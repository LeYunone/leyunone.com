# 大数据写入经典场景问题

## 背景

有做对账、数据同步、转移库等等业务或系统的同学可能都经历一个问题：数据量过大，引起的一系列慢、坏、缺的问题

针对大数据写入这一问题，小到一个方法调用，大到程序设计，在语言程序编写上来看一直以来是一种美学学问

因此，披露大数据写入的细节，对于程序员的我们来说，无非是一种并发理解的享受；

对于这一经典的并发场景问题，一直以来争论的是三点：

- 快
- 事务
- 数据量

不过可惜的是，像是数学中的三角图形一样，无论如何设计都无法去保证这个三角形的绝对完美。

追求快，那么就难保证事务；

追求事务，则无法使之快；

数据量则是两者的催化剂；

![](https://leyunone-img.oss-cn-hangzhou.aliyuncs.com/image/2024-01-04/ef38e539-fc27-4407-893a-df8cd12daedf.png)

但是在各个 “顶点”来看，大数据量写入的需求，可以去结合本身的特殊性进行分类讨论，使之功能趋近完美；

## 分类讨论

### 快

模拟一个只需要快的需求：

> 爬虫，比如领导下达一个任务：帮我把公司所有人在GitLab上的代码提交信息与详情保存记录下来，并且进行每日更新

爬取GitLab每个人的提交记录，可能大家不知道，这确实是领导为了管理总结可以提出的离谱需求，~~至少我已经遇到了~~

首先已知，从Git上拉取下来的数据超10W条，因为只需内部查阅问题，所以不要求数据的准确与稳定。

那么在这个三角形中，事务不保证下的大数据量写入，完美的方案只有快。

对于快来说，大体上有两种对策：

1. 异步插入
2. 数据库移植

#### 异步

